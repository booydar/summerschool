{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренировочных данных очень мало, поэтому хороший результат можно получить, загрузив предобученную сеть, в данном случае была использована ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ayd98/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "num_classes = 2\n",
    "resnet_weights_path = 'resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем модель с аугментацией:\n",
    "\n",
    "добавим поворот, отражение, приближение и сдвиг по высоте и ширине. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n",
      "Epoch 1/36\n",
      "2/2 [==============================] - 11s 5s/step - loss: 0.0348 - acc: 1.0000\n",
      "Epoch 2/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0419 - acc: 1.0000\n",
      "Epoch 3/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 4/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0264 - acc: 1.0000\n",
      "Epoch 5/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0354 - acc: 1.0000\n",
      "Epoch 6/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0325 - acc: 1.0000\n",
      "Epoch 7/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 8/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 9/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 10/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 11/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0317 - acc: 1.0000\n",
      "Epoch 12/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 13/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 14/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0332 - acc: 1.0000\n",
      "Epoch 15/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 16/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 17/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 18/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 19/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0341 - acc: 1.0000\n",
      "Epoch 20/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0351 - acc: 1.0000\n",
      "Epoch 21/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0314 - acc: 1.0000\n",
      "Epoch 22/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 23/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0491 - acc: 1.0000\n",
      "Epoch 24/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 25/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0274 - acc: 1.0000\n",
      "Epoch 26/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0367 - acc: 1.0000\n",
      "Epoch 27/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0239 - acc: 1.0000\n",
      "Epoch 28/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0534 - acc: 1.0000\n",
      "Epoch 29/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 30/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 31/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 32/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 33/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 34/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0615 - acc: 1.0000\n",
      "Epoch 35/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 36/36\n",
      "2/2 [==============================] - 10s 5s/step - loss: 0.0220 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa26415fac8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_height = 256\n",
    "image_width = 341\n",
    "target_size = (image_width, image_height)\n",
    "\n",
    "\n",
    "data_generator_with_aug = ImageDataGenerator(\n",
    "        rotation_range=10, \n",
    "        zoom_range = 0.1, \n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,  \n",
    "        vertical_flip=True)\n",
    "\n",
    "train_generator = data_generator_with_aug.flow_from_directory(\n",
    "        'train',\n",
    "        target_size=target_size,\n",
    "        batch_size=24,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3,\n",
    "        epochs=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В другой версии программы были взяты несколько фотографий для валидации, чтобы более объективно оценивать результат модели. (Результат одинаков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\\nvalidation_generator = data_generator_no_aug.flow_from_directory(\\n        'val',\\n        target_size=target_size,\\n        class_mode='categorical')\\n\\nmodel.fit_generator(\\n        train_generator,\\n        steps_per_epoch=3,\\n        epochs=36,\\n        validation_data=validation_generator,\\n        validation_steps=1)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = data_generator_no_aug.flow_from_directory(\n",
    "        'val',\n",
    "        target_size=target_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3,\n",
    "        epochs=36,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим изображения и преобразуем в массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test_filenames =next(os.walk('test'))\n",
    "test_filenames = sorted(test_filenames)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(660, 341, 256, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = image.load_img('test/' + test_filenames[0], target_size=target_size)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "\n",
    "for name in test_filenames[1:]:\n",
    "    img = image.load_img('test/' + name, target_size=target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([images, x])\n",
    "    \n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим предсказания и оценим их объективность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "доля грязных тарелок =  0.6848484848484848\n"
     ]
    }
   ],
   "source": [
    "classes = model.predict_classes(images, batch_size=30)\n",
    "print(\"доля грязных тарелок = \", sum(classes == 1) / len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned = [i for i in range(len(classes)) if classes[i] == 0]\n",
    "#print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки вручную размечено 34 фотографии из теста для предварительной оценки точности. Проверим точность на них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "угадано грязных  0.9787234042553191\n",
      "угадано чистых  0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "really_cleaned = np.array([5,7,8,13,16,18,19,21,22,23,33])\n",
    "real_dirty = [0 if i in really_cleaned else 1 for i in range(34)]\n",
    "real_clean = [1 if i in really_cleaned else 0 for i in range(34)]\n",
    "\n",
    "\n",
    "print(\"угадано грязных \", f1_score(real_dirty, classes[:34]))\n",
    "print(\"угадано чистых \", f1_score(real_clean, 1 - classes[:34]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что точность довольно высокая, модель адекватна. На кеггле получен результат 0.96969 (ник 01)\n",
    "\n",
    "Преобразуем результаты в таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004</td>\n",
       "      <td>dirty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  label\n",
       "0  0000  dirty\n",
       "1  0001  dirty\n",
       "2  0002  dirty\n",
       "3  0003  dirty\n",
       "4  0004  dirty"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = list(map(lambda x: x[:-4], test_filenames))\n",
    "pred = ['cleaned' if i == 0 else 'dirty' for i in classes]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['id'] = ind\n",
    "\n",
    "data['pred'] = pred\n",
    "data.columns = ['id', 'label']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, результат можно улучшить, подобрав дополнительные параметры для аугментации. Из опробованных текущие показали себя наилучшим образом."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
